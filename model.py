from pyspark.sql.window import Window
from pyspark.sql.functions import lit, lag, avg, asc, col, last, row_number, stddev
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.sql import SparkSession

username = "tmyr"
password = "Qwerty12!"
server_name = "jdbc:sqlserver://technionddscourse.database.windows.net:1433"
database_name = "tmyr"
url = server_name + ";" + "databaseName=" + database_name + ";"

cluster1 = ['FRE00106196', 'FRM00007168', 'FRE00171632', 'FRE00171619', 'FRM00007460', 'FRM00007149', 'FRM00007481',
            'FRE00104112', 'FRE00104040', 'FRE00104949', 'FR000007130', 'FRE00104092', 'FRE00104048', 'FR000007150',
            'FR000007255', 'FRE00104072', 'FRE00104886', 'FRE00104044', 'FRM00007005', 'FRM00007471', 'FRE00104088',
            'FRE00104975', 'FRM00007299', 'FRM00007280', 'FRE00104901', 'FR000007510', 'FRE00106184', 'FRE00104887',
            'FRE00104052', 'FRE00106192', 'FRM00007591', 'FRM00007335', 'FRM00007015', 'FRM00007181', 'FRM00007072',
            'FRM00007558', 'FR069029001', 'FR000007190', 'FRM00061998', 'FRM00007535', 'FRE00104934', 'FRE00104982',
            'FRE00104036', 'FR000007630', 'FRM00007037', 'FRM00007240', 'FRM00007314', 'FRM00007139', 'FRM00007027',
            'FRM00007180', 'FRE00106190', 'FRE00106200']
cluster2 = ['FRM00007643', 'FR013055001', 'FRE00104937', 'FRE00104124', 'FR000007747', 'FRE00106209', 'FRE00104120',
            'FRM00007207', 'FRM00007690', 'FRM00007100', 'FRE00171623', 'FRM00007761', 'FRE00171640', 'FRE00104943',
            'FRE00106207', 'FRM00007222', 'FR000007650', 'FRM00007117', 'FRE00171627', 'FRM00007661', 'FRM00007790']
cluster3 = ['FRE00104963', 'FR000007560']
cluster4 = ['FRM00007607', 'FRE00104980', 'FRE00104936', 'FRE00106205', 'FRM00007627', 'FRE00104957', 'FRE00104974',
            'FRM00007110', 'FRE00106203', 'FRE00104907', 'FRE00104484', 'FRE00104930', 'FRE00104116', 'FRM00007434',
            'FRE00104979', 'FRE00104883', 'FRE00106195', 'FRE00104902', 'FRE00104935', 'FRM00007621']


class LagGather:
    # generates features for machine-learning
    # previous time-step values are used as features
    def __init__(self):
        # this class has 2 data members
        self.nLags = 0
        self.FeatureNames = []

    def setLagLength(self, nLags):
        # this method sets the lag-length
        # if we want only previous time-step as feature
        # use lag-length = 1
        # if we want more lagged time-steps as features
        # set higher lag-length
        self.nLags = nLags
        return self

    def setInputCol(self, colname):
        # sets the input col for which features are generated
        # this identifies the univariate time-series on
        # which machine-learning and forecasting is done
        self.columnName = colname
        return self

    def transform(self, df):
        # transforms the spark dataframe and creates columns
        # that have time-lagged values
        # columns generated as used as features in ML
        df = df.withColumn("Series", lit('Univariate'))
        mywindow = Window.orderBy("Series")
        for i in range(self.nLags):
            strLag = self.columnName + '_LagBy_' + str(i + 1)
            df = df.withColumn(strLag, lag(df[self.columnName], i + 1).over(mywindow))
            self.FeatureNames.append(strLag)
        df = df.drop("Series")
        return df

    def getFeatureNames(self):
        # this return the names of feature-columns that are
        # generated by transform method
        return self.FeatureNames


def forecast(df_train, df_test, forecast_days, regressor):
    """
    :param df_train:train DF
    :param df_test: test DF
    :param forecast_days: day number to forecast (in the future)
    :param regressor: type of regressor
    :return:new DF with label column and predication
    """
    LeadWindow = Window.rowsBetween(0, forecast_days)
    df_train = df_train.withColumn("label", last(df_train["PRCP-tran"]).over(LeadWindow))
    df_test = df_test.withColumn("label", last(df_test["PRCP-tran"]).over(LeadWindow))

    # DECISION-TREE REGRESSOR
    if regressor == "DecisionTreeRegression":
        dr = DecisionTreeRegressor(featuresCol="features", labelCol="label", maxDepth=5)
    # LINEAR REGRESSOR
    if regressor == 'LinearRegression':
        dr = LinearRegression(featuresCol="features", labelCol="label", maxIter=100, regParam=0.4,
                              elasticNetParam=0.1)
    # RANDOM FOREST REGRESSOR
    if regressor == 'RandomForestRegression':
        dr = RandomForestRegressor(featuresCol="features", labelCol="label", maxDepth=5, subsamplingRate=0.8)
    # GRADIENT BOOSTING TREE REGRESSOR
    if regressor == 'GBTRegression':
        dr = GBTRegressor(featuresCol="features", labelCol="label", maxDepth=5, subsamplingRate=0.8)

    model = dr.fit(df_train)
    predictions_dr_test = model.transform(df_test)
    predictions_dr_train = model.transform(df_train)
    return predictions_dr_test, predictions_dr_train


def transform_real_values(df, mean, std, p):
    for i in range(1, p + 1):
        column_old = "PRCP-tran_LagBy_" + str(i)
        column_new = "PRCP-LagBy_" + str(i)
        if i == 0:
            df = df.withColumn(column_new, df[column_old] * std + mean)
        else:
            df = df.withColumn(column_new, df[column_old] * std + mean).drop(column_old)
    return df


def set_features(df, p):
    """
    :param df:Spark DataFrame
    :param p: p parameters in AR model (length of history)
    :return:extract the features from the DF and return new DF with the features and a list with the name of the features
    """
    features = []
    # Auto-regression feature
    LagTransformer = LagGather().setLagLength(p).setInputCol("PRCP-tran")
    df = LagTransformer.transform(df)
    featuresGenerated = LagTransformer.getFeatureNames()
    features.extend(featuresGenerated)
    # VECTOR ASSEMBLER
    # this assembles the all the features
    df = df.dropna()
    vA = VectorAssembler().setInputCols(features).setOutputCol("features")
    df = vA.transform(df)
    return df


def save_predictions(df, regression_type, num_forecast_days, p, filename):
    """
    :param df: spark DataFrame
    :param regression_type:type of regression that been chosen
    :param num_forecast_days:number of days that will be forecast
    :param p: number of days as params
    :param filename: filename of the file that been saved
    :return:
    """
    # order the time series
    df = df.orderBy(col('date').asc())
    rows = df.count()
    # Splitting data into train, test
    df = df.withColumn("Series", lit('Univariate')).withColumnRenamed("PRCP", "PRCP-tran")
    w = Window.orderBy("Series")
    df = df.withColumn("row_num", row_number().over(w))
    df = df.drop("Series")
    split = round(0.7 * rows)
    df_train = df.filter(col("row_num") <= split)
    df_test = df.subtract(df_train).drop("row_num")
    df_train = df_train.drop("row_num")
    # create a DF with the needed features
    df_train = set_features(df_train, p)
    df_test = set_features(df_test, p)


    rmse_test = {}
    rmse_train = {}

    for i in range(num_forecast_days):
        # names of new columns that been added
        new_label = "day_number :" + str(i)
        new_pred = "predication day number :" + str(i)
        evaluator = RegressionEvaluator(predictionCol=new_pred, labelCol=new_label, metricName="rmse")
        # training with Spark's ML algorithms
        df_test, df_train = forecast(df_train, df_test, i, regression_type)
        df_test = df_test.withColumnRenamed("label", new_label)
        df_test = df_test.withColumnRenamed("prediction", new_pred)
        df_train = df_train.withColumnRenamed("label", new_label)
        df_train = df_train.withColumnRenamed("prediction", new_pred)
        rmse_ts = evaluator.evaluate(df_test)
        rmse_tr = evaluator.evaluate(df_train)
        print(f"day :{i}, rmse train : {rmse_tr}, rmse test :{rmse_ts}")
        rmse_test.update({'forecast_' + str(i) + 'day': rmse_ts})
        rmse_train.update({'forecast_' + str(i) + 'day': rmse_tr})
        # predictions for training data
    # Saving data into csv files
    df_test.toPandas().to_csv(filename + 'test.csv', header=True)
    df_train.toPandas().to_csv(filename + 'test.csv', header=True)

    # error statistics summary
    print("Error statistics summary for %s " % filename)
    print("RMSE for train data:\n")
    print(rmse_train)
    print("RMSE for test data:\n")
    print(rmse_test)
    print('Two output files created')
    print('Predictions for train data: %s' % (filename + 'train.csv'))
    print('Predictions for test data: %s' % (filename + 'test.csv'))



if __name__ == "__main__":
    spark = SparkSession.builder.getOrCreate()
    # fetch the station information
    StationDF = spark.read \
        .format("jdbc") \
        .option("url", url) \
        .option("dbtable", "FR_Table") \
        .option("user", username) \
        .option("password", password).load()
    StationDF = StationDF.filter(col('Variable') == 'PRCP').drop("Variable")
    df_1 = StationDF.filter(col('StationId').isin(cluster1) == True).drop("StationId") \
        .groupby('Date').agg(avg("Value").alias("PRCP"))
    #df_2 = StationDF.filter(col('StationId').isin(cluster2) == True).drop("StationId") \
    #    .groupby('Date').agg(avg("Value").alias("PRCP"))
    #df_3 = StationDF.filter(col('StationId').isin(cluster3) == True).drop("StationId") \
    #    .groupby('Date').agg(avg("Value").alias("PRCP"))
    #df_4 = StationDF.filter(col('StationId').isin(cluster4) == True).drop("StationId") \
    #    .groupby('Date').agg(avg("Value").alias("PRCP"))
    regressionType = "LinearRegression"
    forecast_days = 3
    p = 3
    regressionTypeList = ["LinearRegression", "DecisionTreeRegression", 'RandomForestRegression', 'GBTRegression']
    for re in regressionTypeList:
        for pt in [4, 5]:
            print(f"-----------------{re}-------{pt}--------------")
            file_name = re + str(pt)
            save_predictions(df_1, re, forecast_days, pt, file_name)
