from pyspark.sql.window import Window
from pyspark.sql.functions import lit, lag, avg, asc, col, last, first
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from statsmodels.tsa.stattools import adfuller
from pyspark.sql import SparkSession

username = "tmyr"
password = "Qwerty12!"
server_name = "jdbc:sqlserver://technionddscourse.database.windows.net:1433"
database_name = "tmyr"
url = server_name + ";" + "databaseName=" + database_name + ";"


class LagGather:
    # generates features for machine-learning
    # previous time-step values are used as features
    def __init__(self):
        # this class has 2 data members
        self.nLags = 0
        self.FeatureNames = []

    def setLagLength(self, nLags):
        # this method sets the lag-length
        # if we want only previous time-step as feature
        # use lag-length = 1
        # if we want more lagged time-steps as features
        # set higher lag-length
        self.nLags = nLags
        return self

    def setInputCol(self, colname):
        # sets the input col for which features are generated
        # this identifies the univariate time-series on
        # which machine-learning and forecasting is done
        self.columnName = colname
        return self

    def transform(self, df):
        # transforms the spark dataframe and creates columns
        # that have time-lagged values
        # columns generated as used as features in ML
        df = df.withColumn("Series", lit('Univariate'))
        mywindow = Window.orderBy("Series")
        for i in range(self.nLags):
            strLag = self.columnName + '_LagBy_' + str(i + 1)
            df = df.withColumn(strLag, lag(df[self.columnName], i + 1).over(mywindow))
            self.FeatureNames.append(strLag)
        df = df.drop("Series")
        return df

    def getFeatureNames(self):
        # this return the names of feature-columns that are
        # generated by transform method
        return self.FeatureNames


class MovingAverageSmoothing:
    # this class is used for performing Moving-average smoothing
    def __init__(self):
        # this class has 2 data members
        self.nLags = 0
        self.FeatureNames = []

    def setLagLength(self, nLags):
        # this sets the window size over which moving average is performed
        self.nLags = nLags
        return self

    def setInputCol(self, colname):
        # this sets the time-series column on which
        # moving-average is performed
        self.columnName = colname
        return self

    def transform(self, df):
        # this transforms the spark dataframe (i.e time-series column)
        # and creates column contain the moving-average over created
        # time-window
        mywindow = Window.rowsBetween(-self.nLags, 0)
        strMovAvg = self.columnName + '_' + str(self.nLags) + '_MovingAvg'
        df = df.withColumn(strMovAvg, avg(df[self.columnName]).over(mywindow))
        self.FeatureNames.append(strMovAvg)
        return df

    def getFeatureNames(self):
        # this returns the name of feature-column
        # created by transform method
        return self.FeatureNames


def difference(df, inputCol, outputCol, diff):
    # performs first-order differencing
    lag1Window = Window.rowsBetween(-diff, 0)
    df = df.withColumn(outputCol, df[inputCol] - first(df[inputCol]).over(lag1Window))
    return df


def forecast(df, forecast_days, p, q, timeSeriesColumn, regressor):
    # this performs model training
    # this calls the machine-learning algorithms of Spark ML library
    # creating labels for machine-learning
    LeadWindow = Window.rowsBetween(0, forecast_days)
    df = df.withColumn("label", last(df[timeSeriesColumn]).over(LeadWindow))

    features = [timeSeriesColumn]

    # Auto-regression feature
    LagTransformer = LagGather().setLagLength(p).setInputCol(timeSeriesColumn)
    df = LagTransformer.transform(df)
    featuresGenerated = LagTransformer.getFeatureNames()
    features.extend(featuresGenerated)
    # Moving Average params
    # mAvgTransform = MovingAverageSmoothing().setLagLength(q).setInputCol(timeSeriesColumn)
    # df = mAvgTransform.transform(df)
    # featuresGenerated = mAvgTransform.getFeatureNames()
    # features.extend(featuresGenerated)
    # Other feature generators here:
    # Moving Average Smoothing
    # TrendGather
    # VECTOR ASSEMBLER
    # this assembles the all the features
    df = df.dropna()
    vA = VectorAssembler().setInputCols(features).setOutputCol("features")
    df_m = vA.transform(df)
    # Splitting data into train, test
    splitRatio = 0.7
    df_train, df_test = df_m.randomSplit([splitRatio, 1 - splitRatio], seed=12345)
    # DECISION-TREE REGRESSOR
    if regressor == "DecisionTreeRegression":
        dr = DecisionTreeRegressor(featuresCol="features", labelCol="label", maxDepth=5)
    # LINEAR REGRESSOR
    if regressor == 'LinearRegression':
        dr = LinearRegression(featuresCol="features", labelCol="label", maxIter=100, regParam=0.4, elasticNetParam=0.1)
    # RANDOM FOREST REGRESSOR
    if regressor == 'RandomForestRegression':
        dr = RandomForestRegressor(featuresCol="features", labelCol="label", maxDepth=5, subsamplingRate=0.8)
    # GRADIENT BOOSTING TREE REGRESSOR
    if regressor == 'GBTRegression':
        dr = GBTRegressor(featuresCol="features", labelCol="label", maxDepth=5, subsamplingRate=0.8)

    model = dr.fit(df_train)
    predictions_dr_test = model.transform(df_test)
    predictions_dr_train = model.transform(df_train)
    # RMSE is used as evaluation metric
    evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="label", metricName="rmse")
    RMSE_dr_test = evaluator.evaluate(predictions_dr_test)
    RMSE_dr_train = evaluator.evaluate(predictions_dr_train)
    return df_test, df_train, predictions_dr_test, predictions_dr_train, RMSE_dr_test, RMSE_dr_train


def Predict(i, df1, df2, timeSeriesCol, predictionCol, joinCol):
    # this converts differenced predictions to raw predictions
    dZCol = 'DeltaZ' + str(i)
    f_strCol = 'forecast_' + str(i) + 'day'
    df = df1.join(df2, [joinCol], how='inner') \
        .orderBy(asc('Date'))
    df = df.withColumnRenamed(predictionCol, dZCol)
    df = df.withColumn(f_strCol, col(dZCol) + col(timeSeriesCol))
    return df


def CheckStationarity(timeSeriesCol):
    # this function works with Pandas dataframe only not with spark dataframes
    # this performs Augmented Dickey-Fuller's test

    test_result = adfuller(timeSeriesCol.values)
    print('ADF Statistic: % f \n%', test_result[0])
    print('p - value: % f \n%', test_result[1])
    print('Critical values are: \n')
    print(test_result[4])


def SavePredictions(df, timeSeriesCol, regressionType, forecast_days, p, q, filename, diff):
    # this is the main function which calls forecast and predict
    # this saves predictions in csv files

    # Differencing data to remove non-stationarity
    df = df.orderBy(col('date').asc())
    diff_timeSeriesCol = "Diff_" + timeSeriesCol
    df = difference(df, timeSeriesCol, diff_timeSeriesCol, diff)

    RMSE_test = {}
    RMSE_train = {}

    # Forecasting and Undifferencing the data
    for i in range(1, forecast_days + 1):
        # training with Spark's ML algorithms
        df_test, df_train, predictions_test, predictions_train, \
        RMSE_ts, RMSE_tr = forecast(df.select("Date", timeSeriesCol, diff_timeSeriesCol), i, p, q,
                                    diff_timeSeriesCol, regressionType)

        RMSE_test.update({'forecast_' + str(i) + 'day': RMSE_ts})
        RMSE_train.update({'forecast_' + str(i) + 'day': RMSE_tr})
        # predictions for training data
        if i == 1:

            # saving the 1-day forecast as separate column
            corr_predict_train = Predict(i, df_train.select("Date", timeSeriesCol),
                                         predictions_train.select("Date", "prediction"), timeSeriesCol,
                                         "prediction", "Date")

            corr_predict_test = Predict(i, df_test.select("Row Number", "Date", timeSeriesCol),
                                        predictions_test.select("Row Number", "prediction"), timeSeriesCol,
                                        "prediction", "Row Number")
        else:
            # saving each subsequent forecast as separate column
            strCol_prev = "forecast_" + str(i - 1) + "day"
            corr_predict_train = Predict(i, corr_predict_train, predictions_train.select("Row Number",
                                                                                         "prediction"), strCol_prev,
                                         "prediction", "Row Number")
            corr_predict_test = Predict(i, corr_predict_test, predictions_test.select("Row Number",
                                                                                      "prediction"), strCol_prev,
                                        "prediction", "Row Number")
            # saving actual labels as separate columns
            LeadWindow = Window.rowsBetween(0, i)
            a_strCol = "actual_" + str(i) + "day"
            corr_predict_test = corr_predict_test.withColumn(a_strCol, last(corr_predict_test[timeSeriesCol]) \
                                                             .over(LeadWindow))
            corr_predict_train = corr_predict_train.withColumn(a_strCol, last(corr_predict_test[timeSeriesCol]) \
                                                               .over(LeadWindow))
    # Saving data into csv files
    corr_predict_test.write.format("csv").option("header", "true").save(filename + "test.csv")
    corr_predict_train.write.format("csv").option("header", "true").save(filename + "train.csv")

    # error statistics summary
    print("Error statistics summary for %s " % filename)
    print("RMSE for train data:\n")
    print(RMSE_train)
    print("RMSE for test data:\n")
    print(RMSE_test)
    print('Two output files created')
    print('Predictions for train data: %s' % (filename + 'train.csv'))
    print('Predictions for test data: %s' % (filename + 'test.csv'))
    return RMSE_train, RMSE_test


if __name__ == "__main__":
    spark = SparkSession.builder.getOrCreate()
    # fetch the station information
    StationDF = spark.read \
        .format("jdbc") \
        .option("url", url) \
        .option("dbtable", "GB_Table") \
        .option("user", username) \
        .option("password", password).load()
    oneStation = StationDF.filter(col('StationId') == 'GBM00064550').filter(col('Variable') == 'PRCP').drop("Variable")
    timeSeriesCol = "Value"
    regressionType = "LinearRegression"
    forecast_days = 5
    p = 100
    q = 100
    RMSE_train, RMSE_test = SavePredictions(oneStation, timeSeriesCol, regressionType, forecast_days, p, q, "First",
                                            1)
